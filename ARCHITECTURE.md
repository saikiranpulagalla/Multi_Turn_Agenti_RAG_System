# ğŸ—ï¸ Architecture Documentation

This document describes the system architecture and design patterns used in the Agentic RAG System.

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Streamlit UI                            â”‚
â”‚            (streamlit_app.py)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Router Agent                              â”‚
â”‚    (Intelligent Query Routing)                              â”‚
â”‚    â”œâ”€â”€ RAG Mode   (Document Search)                         â”‚
â”‚    â”œâ”€â”€ Web Mode   (Internet Search)                         â”‚
â”‚    â””â”€â”€ Hybrid     (Both)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚               â”‚                  â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ RAG Agent â”‚  â”‚Web Search Ag. â”‚  â”‚Synthesis Ag. â”‚
    â”‚           â”‚  â”‚               â”‚  â”‚              â”‚
    â”‚â€¢ Ingest   â”‚  â”‚â€¢ Query Web    â”‚  â”‚â€¢ Merge Data  â”‚
    â”‚â€¢ Chunk    â”‚  â”‚â€¢ Parse Result â”‚  â”‚â€¢ LLM Synth   â”‚
    â”‚â€¢ Embed    â”‚  â”‚â€¢ Cache        â”‚  â”‚â€¢ Format Out  â”‚
    â”‚â€¢ Retrieve â”‚  â”‚â€¢ Rate Limit   â”‚  â”‚â€¢ Error Hdl   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚             â”‚                   â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚         LLM Client Abstraction             â”‚
    â”‚  â”œâ”€â”€ OpenAI (Primary)                      â”‚
    â”‚  â””â”€â”€ Google Gemini (Fallback)              â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    Supporting Services                 â”‚
    â”‚  â”œâ”€â”€ Vector Store (In-Memory/FAISS)    â”‚
    â”‚  â”œâ”€â”€ Memory Service (Sessions)         â”‚
    â”‚  â”œâ”€â”€ Config Management                 â”‚
    â”‚  â””â”€â”€ Logging & Tracing                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

### 1. Router Agent (`core/agents/router_agent.py`)

**Responsibility**: Intelligent routing of queries to appropriate backends

**Key Features:**
- Analyzes query text, length, and keywords
- Makes routing decisions based on configurable heuristics
- Supports three modes: RAG, Web, Hybrid
- Comprehensive error handling

**Decision Logic:**
```python
# Pseudo-code
if has_doc and query_is_short:
    route = "rag"
elif query_mentions_recent:
    route = "web"
elif query_asks_for_comparison:
    route = "hybrid"
else:
    route = "web" if not has_doc else "hybrid"
```

### 2. RAG Agent (`core/agents/rag_agent.py`)

**Responsibility**: Document ingestion, chunking, and retrieval

**Key Features:**
- PDF parsing with pdfplumber
- Intelligent chunking with overlapping windows
- Embedding generation using sentence-transformers
- Late-chunking retrieval strategy
- Parent-document indexing

**Workflow:**
```
PDF Input
    â†“
Extract Text (page-by-page)
    â†“
Create Chunks (with overlap)
    â†“
Generate Embeddings
    â†“
Store in Vector DB
    â†“
Index Parent Documents
    â†“
Ready for Retrieval
```

### 3. Web Search Agent (`core/agents/web_search_agent.py`)

**Responsibility**: Web search capability and caching

**Key Features:**
- SerpAPI integration for real web search
- Stub responses for demo/testing
- Request caching with TTL
- Error handling and retries
- Statistics tracking

**Flow:**
```
Query
    â†“
Check Cache
    â”œâ”€ Hit â†’ Return Cached Results
    â””â”€ Miss â†’ Call SerpAPI
         â†“
    Parse Results
         â†“
    Cache Results
         â†“
    Return Results
```

### 4. Synthesis Agent (`core/agents/synthesis_agent.py`)

**Responsibility**: Merging and synthesizing results into coherent responses

**Key Features:**
- Prompt engineering for better outputs
- Multi-source synthesis (RAG + Web)
- Structured output format
- Error recovery with helpful messages

**Synthesis Process:**
```
RAG Chunks + Web Results
    â†“
Build Structured Prompt
    â†“
Call LLM with Config Parameters
    â†“
Parse Response
    â†“
Format Output with Metadata
    â†“
Return to User
```

### 5. Vector Store (`core/retriever/vectorstore_adapter.py`)

**Responsibility**: Vector storage and similarity search

**Implementation:**
- In-memory storage using NumPy
- Cosine similarity for ranking
- Support for metadata attachment
- Easy migration path to FAISS/Pinecone

**Operations:**
```python
add(id, embedding, metadata)      # Store vector
search(query_emb, top_k)          # Retrieve similar
get_stats()                        # Get store info
```

### 6. LLM Client (`core/llm_client.py`)

**Responsibility**: Unified LLM interface with fallback

**Key Features:**
- OpenAI support (primary)
- Google Gemini support (fallback)
- Retry logic
- Clear error messages
- Configurable parameters

**Error Handling:**
```
Try OpenAI
    â””â”€ Fail â†’ Try Gemini
         â””â”€ Fail â†’ Raise detailed error
```

## Advanced Retrieval: Late Chunking

### Why Late Chunking?

Traditional RAG uses small chunks for efficiency but loses context. Late chunking combines efficiency with context preservation.

### Implementation

```
Document
    â†“
Split into Large Chunks (2000 chars)
    â”œâ”€ Chunk 1: "Document overview..."
    â”œâ”€ Chunk 2: "Key findings..."
    â””â”€ Chunk 3: "Conclusions..."
         â†“
For Each Large Chunk:
    â”œâ”€ Create embeddings
    â”œâ”€ Store as "parent document"
    â””â”€ Store reference
         â†“
Query Comes In
    â”œâ”€ Generate query embedding
    â”œâ”€ Find similar parent chunks (top-3)
    â”œâ”€ Within each parent, refine to sub-chunks
    â””â”€ Return top-6 most relevant sub-chunks
```

### Advantages

1. **Better Context**: Larger chunks retain document structure
2. **Accurate Retrieval**: Refining within parents improves accuracy
3. **Efficiency**: Fewer similarity computations than naive approach

## Data Flow

### Query Processing Flow

```
User Input (Query + Document Status)
    â”‚
    â”œâ”€â†’ [Router] Decide routing strategy
    â”‚       â””â”€â†’ Analyze query keywords, length
    â”‚       â””â”€â†’ Check document availability
    â”‚       â””â”€â†’ Return: "rag" | "web" | "hybrid"
    â”‚
    â”œâ”€â†’ [Retrieval] Based on route decision:
    â”‚   â”‚
    â”‚   â”œâ”€â†’ RAG Path:
    â”‚   â”‚   â”œâ”€ Encode query to embeddings
    â”‚   â”‚   â”œâ”€ Find parent documents
    â”‚   â”‚   â””â”€ Retrieve top-k chunks
    â”‚   â”‚
    â”‚   â””â”€â†’ Web Path:
    â”‚       â”œâ”€ Check cache
    â”‚       â”œâ”€ Call SerpAPI (if configured)
    â”‚       â””â”€ Return web results
    â”‚
    â”œâ”€â†’ [Synthesis] Merge results
    â”‚   â”œâ”€ Build prompt from chunks + web results
    â”‚   â”œâ”€ Call LLM with temperature & max_tokens
    â”‚   â””â”€ Parse LLM response
    â”‚
    â””â”€â†’ [Output] Format and return results
        â”œâ”€ Summary text
        â”œâ”€ Metadata (sources, counts)
        â””â”€ Debug info (timing, route taken)
```

## Configuration Architecture

### Centralized Config (`core/utils/config.py`)

```python
Config
â”œâ”€ LLM Settings
â”‚  â”œâ”€ OPENAI_API_KEY
â”‚  â”œâ”€ GEMINI_API_KEY
â”‚  â””â”€ Model names
â”œâ”€ Retrieval Settings
â”‚  â”œâ”€ CHUNK_SIZE
â”‚  â”œâ”€ TOP_K_CHUNKS
â”‚  â””â”€ Embedding model
â”œâ”€ Performance Settings
â”‚  â”œâ”€ ENABLE_CACHE
â”‚  â”œâ”€ CACHE_TTL
â”‚  â””â”€ LOG_LEVEL
â””â”€ Validation & Defaults
```

**Benefits:**
- Single source of truth
- Automatic validation on import
- Clear defaults
- Environment variable override

## Error Handling Strategy

### Layered Error Handling

```
Layer 1: Validation
    â”œâ”€ Input validation (empty strings, types)
    â””â”€ Configuration validation (API keys, values)
        â”‚
Layer 2: Try-Catch
    â”œâ”€ File operations (PDF reading)
    â”œâ”€ Network calls (API requests)
    â””â”€ Model operations (embedding, LLM)
        â”‚
Layer 3: Fallback
    â”œâ”€ Use Gemini if OpenAI fails
    â”œâ”€ Use cached results if API fails
    â””â”€ Return meaningful error messages
        â”‚
Layer 4: User Feedback
    â”œâ”€ Clear error messages
    â”œâ”€ Actionable solutions
    â””â”€ Debug information
```

### Example Error Flow

```python
try:
    result = call_openai()
except APIError:
    logger.warning("OpenAI failed, trying Gemini")
    try:
        result = call_gemini()
    except APIError:
        logger.error("All LLMs failed")
        return helpful_error_message()
```

## Performance Considerations

### Caching Strategy

```
User Query
    â”œâ”€ Check Cache
    â”‚  â”œâ”€ Hit (< 1s) â†’ Return cached result
    â”‚  â””â”€ Miss â†’ Proceed
    â”œâ”€ Check Vector DB
    â”‚  â”œâ”€ Cached (1-2s) â†’ Return
    â”‚  â””â”€ New â†’ Generate & cache
    â””â”€ Call LLM
       â””â”€ Cache result (optional)
```

### Optimization Points

1. **Embedding Caching**: Store embeddings, don't regenerate
2. **Query Caching**: Cache synthesis results
3. **Batch Operations**: Process multiple chunks together
4. **Model Optimization**: Use efficient embedding model
5. **Async Operations**: (Future) Support concurrent requests

## Extensibility

### Adding New LLM Providers

```python
# core/llm_client.py
def call_llm(...):
    # Try OpenAI
    try:
        return call_openai(...)
    except:
        pass
    
    # Try Gemini
    try:
        return call_gemini(...)
    except:
        pass
    
    # Add new provider here
    try:
        return call_anthropic(...)
    except:
        pass
    
    raise Exception("All providers failed")
```

### Adding New Agents

```python
# Create new agent
class CustomAgent:
    def process(self, input_data):
        # Custom logic
        return output
        
# Register with router
router = RouterAgent(
    rag_agent=rag,
    web_agent=web,
    custom_agent=custom,  # Add new
    synth_agent=synth
)
```

### Using Different Vector DBs

```python
# Replace InMemoryVectorStore with:
from langchain.vectorstores import FAISS
# or
from pinecone import Index

# Update RAGAgent initialization
rag = RAGAgent(vectorstore=FAISSStore(), ...)
```

## Scalability Path

### Current (Development)
- In-memory storage
- Single process
- No distributed caching

### Near-term (Production)
- FAISS vector store
- Redis caching
- Containerized deployment

### Long-term (Enterprise)
- Pinecone/Weaviate vector DB
- Distributed architecture
- Multi-region deployment
- Advanced analytics

## Security Considerations

1. **API Key Management**
   - Never log API keys
   - Use .env files (not in version control)
   - Rotate keys regularly

2. **Data Privacy**
   - Process user documents locally
   - Don't send user data to external services unnecessarily
   - Clear cache appropriately

3. **Rate Limiting**
   - Implement request throttling
   - Cache to reduce API calls
   - Monitor usage

## Testing Strategy

```
Unit Tests
â”œâ”€ Component tests (each agent)
â”œâ”€ Config validation
â””â”€ Error handling

Integration Tests
â”œâ”€ End-to-end workflows
â”œâ”€ Multi-agent coordination
â””â”€ Error recovery

Performance Tests
â”œâ”€ Throughput benchmarks
â”œâ”€ Latency measurements
â””â”€ Memory profiling
```

---

**Architecture Version**: 1.0  
**Last Updated**: 2024  
**Status**: Production-Ready
