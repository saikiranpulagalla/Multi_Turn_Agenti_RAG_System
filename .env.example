# ============================================================
# Agentic RAG System Configuration
# Copy this file to .env and fill in your values
# ============================================================

# ============================================================
# LLM Configuration (at least one is required)
# ============================================================

# OpenAI API Key - https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-key-here
OPENAI_MODEL=gpt-4o-mini

# Google Gemini API Key - https://ai.google.dev/
GEMINI_API_KEY=your-gemini-key-here
GEMINI_MODEL=gemini-2.0-flash

# ============================================================
# Embedding Configuration
# ============================================================

# Sentence-transformers embedding model
# Options: all-MiniLM-L6-v2 (default, fast, 384-dim), all-mpnet-base-v2 (better, 768-dim), all-roberta-large-v1 (best, 1024-dim)
EMBED_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2

# Embedding dimension (must match your model and Pinecone index)
# all-MiniLM-L6-v2 = 384, all-mpnet-base-v2 = 768, all-roberta-large-v1 = 1024
EMBED_DIM=384

# ============================================================
# Document Processing
# ============================================================

# Chunk size in characters (larger = more context but slower)
CHUNK_SIZE=1000

# Overlap between chunks (helps preserve context across chunks)
CHUNK_OVERLAP=100

# ============================================================
# Retrieval Configuration
# ============================================================

# Number of parent documents to consider
TOP_K_PARENTS=3

# Number of final chunks to use for synthesis
TOP_K_CHUNKS=6

# ============================================================
# Vector Store Configuration (Pinecone)
# ============================================================

# Use Pinecone vector store (default: true)
USE_PINECONE=true

# Pinecone API Key - https://www.pinecone.io/
PINECONE_API_KEY=your-pinecone-api-key-here

# Pinecone index host (e.g., https://rag-xxx.svc.aped-4627-b74a.pinecone.io)
PINECONE_HOST=https://your-pinecone-host-here

# Pinecone index name
PINECONE_INDEX_NAME=rag

# ============================================================
# LLM Generation Parameters
# ============================================================

# Temperature for LLM (0.0 = deterministic, 1.0 = creative)
# Use 0.0 for reliable, factual responses
LLM_TEMPERATURE=0.0

# Maximum tokens in LLM response
LLM_MAX_TOKENS=1024

# ============================================================
# Web Search Configuration (Optional)
# ============================================================

# SerpAPI Key for web search - https://serpapi.com/
# Leave empty to use stub responses instead
SERP_API_KEY=your-serpapi-key-here

# Number of web results to retrieve
WEB_SEARCH_RESULTS=5

# ============================================================
# LangSmith Tracing Configuration (Optional)
# ============================================================

# Enable LangSmith tracing for observability
ENABLE_LANGSMITH=true

# LangSmith API Key - https://smith.langchain.com/
LANGSMITH_API_KEY=your-langsmith-api-key-here

# LangSmith project name
LANGSMITH_PROJECT=agentic-rag

# ============================================================
# Performance Configuration
# ============================================================

# Enable query response caching
ENABLE_CACHE=true

# Cache validity duration in seconds
CACHE_TTL_SECONDS=3600

# ============================================================
# Logging Configuration
# ============================================================

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Enable detailed tracing
ENABLE_TRACING=true

# ============================================================
# Usage Examples:
# ============================================================
# 
# 1. Minimal Setup (OpenAI + In-Memory Store):
#    OPENAI_API_KEY=your-key
#    USE_PINECONE=false
#    ENABLE_LANGSMITH=false
#
# 2. Minimal Setup (Gemini + In-Memory Store):
#    GEMINI_API_KEY=your-key
#    USE_PINECONE=false
#    ENABLE_LANGSMITH=false
#
# 3. Full Setup (All features with Pinecone + LangSmith):
#    OPENAI_API_KEY=your-key
#    GEMINI_API_KEY=your-key
#    PINECONE_API_KEY=your-key
#    PINECONE_HOST=your-host
#    LANGSMITH_API_KEY=your-key
#    SERP_API_KEY=your-key
#
# 4. Production Setup (Optimized):
#    CHUNK_SIZE=1500
#    TOP_K_CHUNKS=8
#    LLM_MAX_TOKENS=2048
#    USE_PINECONE=true
#    ENABLE_LANGSMITH=true
#    ENABLE_CACHE=true
#    LOG_LEVEL=WARNING
# ============================================================
